# -*- coding: utf-8 -*-
"""sentimentAnalysis (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ul5zlBAhUZUgY8WIxgBTkMV7-Xs-mQkT
"""

# !nvidia-smi

from __future__ import absolute_import,division,print_function,unicode_literals

# try :
#   !pip uninstall tb-nightly tensorboardx tensorboard
#   !pip install tf-nightly
# except Exception:
#   pass

import tensorflow as tf
import os
import datetime
import tensorflow_datasets as tfds

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard

import pkg_resources
for entry_point in pkg_resources.iter_entry_points('tensorboard_plugins'):
  print(entry_point.dist)

#!ls -alrt /usr/local/lib/python3.6/dist-packages/tensorboard*

# ! rm -r /usr/local/lib/python3.6/dist-packages/tensorboardcolab-0.0.22.dist-info

print(tf.__version__)

tf.config.experimental.list_physical_devices()

dataset,info = tfds.load('amazon_us_reviews/Mobile_Electronics_v1_00',with_info= True)

train_dataset = dataset['train']

info

len(list(train_dataset))

print(train_dataset)

BUFFER_SIZE = 30000
BATCH_SIZE = 128

train_dataset = train_dataset.shuffle(BUFFER_SIZE,reshuffle_each_iteration=False)

for reviews in train_dataset.take(1):
  print(reviews)

for reviews in train_dataset.take(5):
  review_text = reviews['data']
  print(review_text.get('review_body').numpy())
  print(review_text.get('star_rating'))
  print(tf.where(review_text.get('star_rating')>3,1,0).numpy())

tokenizer = tfds.features.text.Tokenizer()
vocabulary_set= set()
for _,reviews in train_dataset.enumerate():
  review_text = reviews['data']
  reviews_tokens = tokenizer.tokenize(review_text.get('review_body').numpy())
  vocabulary_set.update(reviews_tokens)

vocab_size=len(vocabulary_set)
vocab_size

print(vocabulary_set)

encoder = tfds.features.text.TokenTextEncoder(vocabulary_set)

for reviews in train_dataset.take(2):
    review_text = reviews['data']
    enc_exp= encoder.encode(review_text.get('review_body').numpy())
    print(review_text.get('review_body').numpy())
    print(enc_exp)

for index in enc_exp :
  print('{}----->{}'.format(index,encoder.decode([index])))

def encode(text_tensor,label_tensor):
  encoded_text = encoder.encode(text_tensor.numpy())
  label = tf.where(label_tensor>3,1,0)
  return encoded_text, label

def encode_map_fn(tensor):
  text = tensor['data'].get('review_body')
  label = tensor['data'].get('star_rating')
  encoded_text,label = tf.py_function(encode,inp=[text,label],Tout=(tf.int64,tf.int32))
  encoded_text.set_shape([None])
  label.set_shape([])
  return encoded_text , label

ar_encoded_data = train_dataset.map(encode_map_fn)

for fo,f1 in ar_encoded_data.take(2) :
  print('{}------->{}'.format(fo,f1))

TAKE_SIZE = 10000
train_data = ar_encoded_data.skip(TAKE_SIZE).shuffle(BUFFER_SIZE)
train_data = train_data.padded_batch(BATCH_SIZE)
test_data = ar_encoded_data.take(TAKE_SIZE)
test_data = test_data.padded_batch(BATCH_SIZE)

vocab_size+=1

sample_text , sample_label = next(iter(test_data))
sample_text[0],sample_label[0]

for fo, f1 in test_data.take(2):
  print(tf.unique_with_counts(f1)[2].numpy())



model=tf.keras.Sequential()
model.add(tf.keras.layers.Embedding(vocab_size,128))
model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128,return_sequences= True)))
model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,return_sequences= True)))
model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)))
model.add(tf.keras.layers.Dense(64,activation='relu'))
model.add(tf.keras.layers.Dense(64,activation='relu'))
model.add(tf.keras.layers.Dense(1))

# !rm -r /tmp/logs/

logdir=os.path.join("/tmp/logs" ,datetime.datetime.now().strftime("%Y%m%d-%H%M%S"))
tensorboard_callback =tf.keras.callbacks.TensorBoard(logdir,histogram_freq=1)
checkpointer = tf.keras.callbacks.ModelCheckpoint(filepath='/temp/sentiment_analysis.hdf5',verbose=1,save_best_only=True)
model.compile(optimizer='adam',loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),metrics=['accuracy'])
history = model.fit(train_data,epochs=4,validation_data=test_data,callbacks=[tensorboard_callback,checkpointer])
model.save('/tmp/final_sentiment_analysis.hdf5')



!ls -alrt /

eval_loss, eval_acc = model.evaluate(test_data)
print('/nEval_loss:{:.3f},Eval_acc:{:.3f}'.format(eval_loss,eval_acc) )

for fo,f1 in test_data.take(2):
  print(f1)
  print(model.predict(fo))

model.layers

model.summary()

model.get_layer('embedding').output

# !nvdia-smi

import matplotlib as plt
def plot_graph(history,metric):
  plt.plot(history.hstory['meric'])

"""# New Section"""